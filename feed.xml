<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://odarasimi.github.io/pages/odarasimi/feed.xml" rel="self" type="application/atom+xml" /><link href="https://odarasimi.github.io/pages/odarasimi/" rel="alternate" type="text/html" /><updated>2024-06-01T17:00:45+00:00</updated><id>https://odarasimi.github.io/pages/odarasimi/feed.xml</id><title type="html">Donuts, Simplices, and Conics</title><subtitle>Let&apos;s talk about Geometry, Cameras. Conics, Manifolds</subtitle><author><name>Anuoluwapo Adisa</name></author><entry><title type="html">PyTorch is Ten out of ATen</title><link href="https://odarasimi.github.io/pages/odarasimi/programming/2024/06/01/PyTorch-is-Ten-out-of-ATen.html" rel="alternate" type="text/html" title="PyTorch is Ten out of ATen" /><published>2024-06-01T16:05:53+00:00</published><updated>2024-06-01T16:05:53+00:00</updated><id>https://odarasimi.github.io/pages/odarasimi/programming/2024/06/01/PyTorch-is-Ten-out-of-ATen</id><content type="html" xml:base="https://odarasimi.github.io/pages/odarasimi/programming/2024/06/01/PyTorch-is-Ten-out-of-ATen.html"><![CDATA[<p><em>“I have made this letter longer than usual, only because I have not had the time to make it shorter.” - Blaise Pascal</em></p>

<p>For anyone with passing familiarity with deep learning workloads and the challenge of scaling, it is generally no surprise to see AI companies gathered at the cathedral of accelerated GPU hardware. Personally, I think domain-centric hardware could be the future, or perhaps compilers that support various backends, like XLA <a href="https://openxla.org/xla">https://openxla.org/xla</a>. 
Nevertheless, hand-woven optimization remains integral (no pun intended) in areas like geometry processing and deep learning.
These are some notes I took in an attempt to figure out C++ (and consequently CUDA) for PyTorch. They could perhaps be more concise, but brevity was sacrificed on the altar of clarity, and I hope they serve to aid understanding.</p>

<h6 id="motivation-and-obligatory-praise-aten">Motivation and obligatory praise: ATen</h6>
<p>ATen is a foundational library in the PyTorch ecosystem that provides the fundamental building blocks for tensor operations, memory management, and data types. Simply put - it powers crucial Pytorch operations. ATen is implemented in C++ and forms the <strong>backend</strong> for many PyTorch operations.</p>

<p>ATen also provides a set of optimized tensor operations that are written in C++, and these include element-wise arithmetic, matrix operations, reductions, etc. These low-level operations are used to build higher-level functions in PyTorch, meaning that many of the higher-level functions we use in PyTorch are powered by ATen. ATen also provides an abstraction layer for datatypes and hides the underlying details of the hardware (CPU, GPU, etc.) from higher-level PyTorch code. This allows PyTorch to run seamlessly on different hardware without extensive changes to the higher-level code.
ATen allows you to define custom C++ tensor operations that can be used within PyTorch, and thus tensor operations are highly optimized and designed for performance. 
This efficiency helps speed up deep learning computations, and this is what we will focus on.</p>

<h6 id="pytorch-and-aten">PyTorch and ATen</h6>
<p>PyTorch uses C++ extensively in its backend, and when one thinks of the relationship between PyTorch and ATen, there are 2 salient points to keep in mind</p>

<ul>
  <li>When you perform tensor operations in PyTorch, the higher-level Python code often triggers ATen’s C++ functions behind the scenes.</li>
  <li>These C++ functions are implemented to perform the actual mathematical computations, memory allocation, and other low-level tasks required for tensor operations.</li>
</ul>

<p>PyTorch’s backend, including ATen, leverages C++ to provide efficient tensor operations, memory management, and hardware abstraction. When you execute PyTorch code involving tensor operations, the relevant C++ functions from ATen are used directly, without some separate compilation step to “ATen code.” This is an orchestra of Python’s high-level expressiveness and C++’s performance optimization.
So whenever we want to optimize our code with GPUs, we provide custom C++ code that we choose to call <em>C++ extensions</em> (using the ATen library directly for tensor computations), and we connect our familiar PyTorch frontend (Python) to our custom C++ code with python bindings (we have to connect the frontend to the backend ourselves, because they are custom)…as a consequence we skip the overhead of the python interpreter.</p>

<p><strong>How do we do all this?</strong></p>
<h6 id="to-integrate-c-with-pytorch">To Integrate C++ with PyTorch</h6>
<ul>
  <li>Write C++ Code: You write C++ code that contains the functionality you need.</li>
  <li>Compile C++ Code: The C++ code is compiled using a C++ compiler, which generates compiled machine code that can be executed by the CPU.</li>
  <li>Python Wrapper: You create a Python wrapper using tools like pybind11. This wrapper defines Python functions that map to the compiled C++ functions.</li>
  <li>Compile Wrapper: The Python wrapper code is compiled into a Python extension module.</li>
  <li>Python Interaction: When you call a Python function from the extension module, the Python interpreter uses the wrapper to call the corresponding compiled C++ function directly. This means that the compiled machine code of the C++ function is executed.</li>
  <li>C++ Execution: The compiled C++ code executes, performing the desired operations.
Return to Python: Control returns to the Python interpreter after the C++ function completes, and any results or return values from the C++ function are returned to Python.</li>
</ul>

<h6 id="c-and-pytorch">C++ and PyTorch</h6>
<p><strong>Ahead of Time with setuptools:</strong></p>
<ul>
  <li>C++ Extensions: C++ extensions are modules that contain C++ code linked with Python, allowing you to use compiled C++ functionality within Python.</li>
  <li>setuptools: setuptools is a package used to build and distribute Python packages. It includes tools for compiling and installing Python extensions.</li>
  <li>Building Ahead of Time: Building “ahead of time” means that you pre-compile your C++ code into a shared library (like a .so file) before running your Python program.</li>
  <li>Process:
    <ul>
      <li>You write your C++ code and create a Python wrapper using tools like pybind11 or Cython.</li>
      <li>You use setuptools to build the C++ extension into a shared library.
This compiled extension module can then be imported and used in your Python code like any other Python module.</li>
    </ul>
  </li>
</ul>

<p><strong>Just in Time with torch.utils.cpp_extension.load():</strong></p>
<ul>
  <li>C++ Extensions: Same as above, C++ extensions are modules containing compiled C++ functionality.
torch.utils.cpp_extension.load(): This is a PyTorch utility function that allows you to compile and load C++ extensions dynamically at runtime, rather than pre-compiling them before running your Python program.</li>
  <li>Building Just in Time: With this approach, you don’t pre-compile the C++ extension ahead of time. Instead, you use torch.utils.cpp_extension.load() to compile and load the extension when needed.</li>
  <li>Process:
    <ul>
      <li>You write your C++ code and create a Python wrapper.</li>
      <li>You use torch.utils.cpp_extension.load() to compile the C++ code and load the extension module dynamically into your Python program at runtime.
This extension module is then available for use in your Python code.</li>
    </ul>
  </li>
</ul>

<p>Ultimately, we can then mix C++ with CUDA kernels (which run on the GPU). The compilers for both C++ AND CUDA each handle their respective pieces of code…and as usual each function is accessible from the Pytorch python frontend.</p>

<p><em>References</em>
<a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">https://pytorch.org/tutorials/advanced/cpp_extension.html</a></p>]]></content><author><name>Anuoluwapo Adisa</name></author><category term="Programming" /><summary type="html"><![CDATA[“I have made this letter longer than usual, only because I have not had the time to make it shorter.” - Blaise Pascal]]></summary></entry><entry><title type="html">Introductory thoughts on Projective Geometry</title><link href="https://odarasimi.github.io/pages/odarasimi/geometry/2024/05/30/Introductory-thoughts-on-projective-geometry.html" rel="alternate" type="text/html" title="Introductory thoughts on Projective Geometry" /><published>2024-05-30T11:10:53+00:00</published><updated>2024-05-30T11:10:53+00:00</updated><id>https://odarasimi.github.io/pages/odarasimi/geometry/2024/05/30/Introductory-thoughts-on-projective-geometry</id><content type="html" xml:base="https://odarasimi.github.io/pages/odarasimi/geometry/2024/05/30/Introductory-thoughts-on-projective-geometry.html"><![CDATA[<p>…</p>

<p><em>“No man ever steps in the same river twice, for it’s not the same river and he’s not the same man.”</em> - Heraclitus</p>

<p><img src="/title_proj_geo.png" alt="projective geometry" width="600" height="400" /></p>

<p>When an object is transformed in some way, we observe a change in the object…but does the object really change, or does our perception of the object change…not to get into philosophical rambling, but I believe the question of perception and change lends key insight into the nature of projective geometry.</p>

<p>I have always had a view of projective geometry, which then extended to two, basically I see projective geometry as:</p>

<ul>
  <li>The study of properties that are invariant under projective transformation.</li>
  <li>The study of lower-dimensional objects through a higher-dimensional lens.</li>
</ul>

<h5 id="the-study-of-invariant-properties-under-projective-transformation">The Study of Invariant Properties under Projective Transformation</h5>

<p>We can accurately capture the essence of abstraction in mathematics, illustrating how specific operations and entities in arithmetic and geometry are generalized into the broader frameworks of algebra and geometry. This abstraction allows mathematicians to study more complex and general properties beyond specific instances. In this context, you have a group of things that follow a specific rule, and you define an operation on the members of that group based on the premise that the members obey the laws that define the structure of the group.</p>

<p><img src="/abstract.png" alt="abstract" width="450" height="250" /></p>

<p>Taking a cue from Kneebone’s ‘Algebraic Projective Geometry’, we see that if numbers and polynomials both satisfy the laws of algebra, then they share a common structure, and we may study a system based on that structure without going into the specifics of the nature of the objects involved other than the fact that they all possess the structure in question.
<em>Aside: This is similar to the ideas in Linear Algebra, where vectors can be of different forms, or live in different spaces. Polynomials, real numbers, and geometric objects are different objects, but they share the fundamental structure that defines linear algebra. Even though we can intuit one from the other (looking at you - dot products), we do not need to project the properties of one onto the other. Thus, when we talk about the “length of a polynomial” via inner products, we are taking a liberty from geometric vector spaces.</em></p>

<p>Now, in Euclidean geometry, rotations and scalings, for example, have a common spatial structure - they both preserve shape. Generally speaking, displacements possess a common spatial structure. A translated, rotated square is considered to be the same square, i.e., the shape is invariant. Euclidean geometry is thus the invariant theory of the group of displacements, and based on these invariants, we define a metric—a way to measure distances, angles, and other structural aspects of the geometry. In a way, these operations define a space, as the Euclidean transformations limit you to the Euclidean space. The same analogy can be carried over to projective geometry, which we define as the invariant theory of the group of projections. That is, under projective transformations, find out which properties of the object remain invariant, and then we define a metric based on those properties.</p>

<h5 id="the-study-of-lower-dimensional-objects-through-a-higher-dimensional-lens">The Study of Lower-Dimensional Objects through a Higher-Dimensional Lens</h5>

<p><img src="/parallel.png" alt="parralel" width="650" height="250" /></p>

<p>On first contact, the statement: “Parallel lines meet at a point in infinity” seems rather absurd, but observing the image above, we see 2 lines (which we should treat as lines with no limitations on length), and we notice that with the rotation of the red line about the point “x”, the point of intersection of the two lines “p”, moves further and further along the blue line, and as the red line gets closer to being parallel with the blue line, it appears to be that the point “p” approaches an “infinite” point along that blue line. Thinking about the oft-used “railway tracks” example, we need to consider the fact that those lines live on a plane, and we live in 3D space. This means that we have the liberty and freedom of an extra dimension. This extra dimension offers us insight through a different view of the same object. The parallelism of the two lines says nothing about the 3D space in which they live, but the 3D space is in no way compelled to say nothing about the 2D objects in its purview.
I plan to discuss more on this and the concept of projective equivalence in a follow-up post.</p>

<p><img src="/projtwod.png" alt="projtwod" width="450" height="250" /></p>

<h6 id="references">References</h6>
<ul>
  <li>
    <p>Kneebone, G. T. (1960). Algebraic Projective Geometry. Oxford: Clarendon Press.</p>
  </li>
  <li>
    <p>Hartley, R., &amp; Zisserman, A. (2004). Multiple View Geometry in Computer Vision (2nd ed.). Cambridge: Cambridge University Press.</p>
  </li>
</ul>]]></content><author><name>Anuoluwapo Adisa</name></author><category term="geometry" /><summary type="html"><![CDATA[…]]></summary></entry></feed>